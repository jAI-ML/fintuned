{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sAAnPAMYohu9"
      },
      "outputs": [],
      "source": [
        "# Install dependencies (run once)\n",
        "!pip install -q transformers==4.35.0 peft accelerate bitsandbytes datasets\n",
        "# Optional: install sentencepiece if tokenizer needs it\n",
        "!pip install -q sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lV-evinZoksP"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "from pathlib import Path\n",
        "import torch\n",
        "from datasets import Dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "# Directories\n",
        "BASE_DIR = '/content/chatbot_delivery'\n",
        "os.makedirs(BASE_DIR, exist_ok=True)\n",
        "os.makedirs(os.path.join(BASE_DIR, 'data'), exist_ok=True)\n",
        "os.makedirs(os.path.join(BASE_DIR, 'result'), exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K5NwZ-C_om4W"
      },
      "outputs": [],
      "source": [
        "# Create & save the mock dataset (20 Q&A pairs)\n",
        "faq = [\n",
        "  {\n",
        "    \"instruction\": \"What is the waiting period for health insurance claims?\",\n",
        "    \"output\": \"The waiting period is typically 30 days for new claims; some plans may have longer periods for pre-existing conditions.\"\n",
        "  },\n",
        "  {\n",
        "    \"instruction\": \"Does my car insurance cover flood damage?\",\n",
        "    \"output\": \"Flood damage is usually covered under comprehensive coverage; collision coverage alone won't cover floods.\"\n",
        "  },\n",
        "  {\n",
        "    \"instruction\": \"How can I add a family member to my health insurance?\",\n",
        "    \"output\": \"Contact your insurer or agent and request to add a dependent; you'll need personal details and proof of relationship.\"\n",
        "  },\n",
        "  {\n",
        "    \"instruction\": \"What is deductible in an insurance policy?\",\n",
        "    \"output\": \"A deductible is the amount you pay out-of-pocket before insurance pays for a claim.\"\n",
        "  },\n",
        "  {\n",
        "    \"instruction\": \"Are pre-existing conditions covered by health insurance?\",\n",
        "    \"output\": \"Coverage depends on the policy; some insurers apply waiting periods for pre-existing conditions.\"\n",
        "  },\n",
        "  {\n",
        "    \"instruction\": \"How do I file a car insurance claim after an accident?\",\n",
        "    \"output\": \"Contact your insurer immediately, document the scene, gather police reports, and submit the claim through the insurer's portal.\"\n",
        "  },\n",
        "  {\n",
        "    \"instruction\": \"Can I cancel my policy anytime?\",\n",
        "    \"output\": \"Yes, you can usually cancel, but cancellation terms (refunds/penalties) depend on the insurer and policy.\"\n",
        "  },\n",
        "  {\n",
        "    \"instruction\": \"What factors affect life insurance premium?\",\n",
        "    \"output\": \"Age, health, tobacco use, occupation, and coverage amount are common factors affecting premiums.\"\n",
        "  },\n",
        "  {\n",
        "    \"instruction\": \"Is roadside assistance included in car insurance?\",\n",
        "    \"output\": \"Roadside assistance is often an optional add-on; check your policy details to confirm.\"\n",
        "  },\n",
        "  {\n",
        "    \"instruction\": \"How long does it take to settle an insurance claim?\",\n",
        "    \"output\": \"Settlement time varies \\u2014 simple claims may take days; complex claims can take weeks or months.\"\n",
        "  },\n",
        "  {\n",
        "    \"instruction\": \"Does health insurance cover mental health services?\",\n",
        "    \"output\": \"Many modern health plans include mental health coverage; check your benefits for specifics.\"\n",
        "  },\n",
        "  {\n",
        "    \"instruction\": \"What is no-claim bonus (NCB)?\",\n",
        "    \"output\": \"NCB is a discount on your premium offered when you make no claims during the policy period.\"\n",
        "  },\n",
        "  {\n",
        "    \"instruction\": \"How to update my contact details with insurance provider?\",\n",
        "    \"output\": \"Log into the insurer portal or contact customer support to update contact information.\"\n",
        "  },\n",
        "  {\n",
        "    \"instruction\": \"Are natural disasters covered under home insurance?\",\n",
        "    \"output\": \"Standard home policies may exclude certain disasters; flood and earthquake coverages are often separate add-ons.\"\n",
        "  },\n",
        "  {\n",
        "    \"instruction\": \"What documents are required for life insurance claim?\",\n",
        "    \"output\": \"Typically death certificate, identity documents, policy copy, and claim forms; check insurer checklist.\"\n",
        "  },\n",
        "  {\n",
        "    \"instruction\": \"Can I get a discount for bundling policies?\",\n",
        "    \"output\": \"Many insurers offer multi-policy discounts when you bundle auto, home, or other policies.\"\n",
        "  },\n",
        "  {\n",
        "    \"instruction\": \"Does my policy have cashless hospitalisation?\",\n",
        "    \"output\": \"If your insurer network includes the hospital, cashless treatment is usually available; verify network hospitals.\"\n",
        "  },\n",
        "  {\n",
        "    \"instruction\": \"What is subrogation in insurance?\",\n",
        "    \"output\": \"Subrogation allows an insurer to pursue recovery from a third party responsible for loss after paying the claim.\"\n",
        "  },\n",
        "  {\n",
        "    \"instruction\": \"How do I renew my insurance policy online?\",\n",
        "    \"output\": \"Visit the insurer website, go to the policy section, and choose renew; you can pay online using provided options.\"\n",
        "  },\n",
        "  {\n",
        "    \"instruction\": \"Can I change my premium payment frequency?\",\n",
        "    \"output\": \"Yes, many insurers allow monthly/quarterly/annual payment options \\u2014 talk to your agent or update online.\"\n",
        "  }\n",
        "]\n",
        "with open(os.path.join(BASE_DIR, 'data', 'faq_sample.json'), 'w') as f:\n",
        "    json.dump(faq, f, indent=2)\n",
        "print('Saved dataset to', os.path.join(BASE_DIR, 'data', 'faq_sample.json'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rNPApUmzopHn"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# ‡™Ö‡™π‡´Ä‡™Ç ‡™§‡™Æ‡™æ‡™∞‡´ã Hugging Face token paste ‡™ï‡™∞‡´ã\n",
        "login(\"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\")\n",
        "\n",
        "model_name = 'mistralai/Mistral-7B-Instruct-v0.2'\n",
        "\n",
        "# Model access ‡™Æ‡™æ‡™ü‡´á token ‡™™‡™æ‡™∏ ‡™ï‡™∞‡´ã\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=True)\n",
        "\n",
        "# ‡™ú‡´ã pad token ‡™® ‡™π‡´ã‡™Ø ‡™§‡´ã EOS token use ‡™ï‡™∞‡´ã\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "print('Tokenizer loaded. Example token:', tokenizer('Hello'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dG20tF2Jor7f"
      },
      "outputs": [],
      "source": [
        "!pip install bitsandbytes accelerate transformers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l_Te4fW6ovcB"
      },
      "outputs": [],
      "source": [
        "# 1. Token login\n",
        "from huggingface_hub import login\n",
        "login(\"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\")  # yaha apna token paste karo\n",
        "\n",
        "# 2. Model name define karo\n",
        "model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "\n",
        "# 3. Model load\n",
        "from transformers import AutoModelForCausalLM\n",
        "\n",
        "print('Attempting to load base model. This step may use a lot of VRAM.')\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    load_in_4bit=True,    # 4-bit quantization\n",
        "    device_map='auto'\n",
        ")\n",
        "print('Base model loaded successfully!')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a229X2y7ozgw"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM\n",
        "print('Attempting to load base model. This step may use a lot of VRAM.')\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    load_in_4bit=True,          # use 4-bit quantization (requires bitsandbytes & supported GPU)\n",
        "    device_map='auto'\n",
        ")\n",
        "print('Base model loaded (or error shown by HF if not supported).')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vSiNhMybo3Bb"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "BASE_DIR = os.getcwd()  # current directory path\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_mNRSNTuo5po"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "from pathlib import Path\n",
        "import torch\n",
        "from datasets import Dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "# Directories\n",
        "BASE_DIR = '/content/chatbot_delivery'\n",
        "os.makedirs(BASE_DIR, exist_ok=True)\n",
        "os.makedirs(os.path.join(BASE_DIR, 'data'), exist_ok=True)\n",
        "os.makedirs(os.path.join(BASE_DIR, 'result'), exist_ok=True)\n",
        "\n",
        "with open(os.path.join(BASE_DIR, 'data', 'faq_sample.json')) as f:\n",
        "    faq_list = json.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_yhco6Kfo8K3"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "from datasets import Dataset\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Define base directory and file path\n",
        "BASE_DIR = '/content/chatbot_delivery'\n",
        "json_path = os.path.join(BASE_DIR, 'data', 'faq_sample.json')\n",
        "\n",
        "# Load JSON data\n",
        "with open(json_path, 'r') as f:\n",
        "    faq_list = json.load(f)\n",
        "\n",
        "# Convert to Hugging Face Dataset\n",
        "ds = Dataset.from_list(faq_list)\n",
        "\n",
        "# Load tokenizer (GPT-2 in this example)\n",
        "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
        "\n",
        "# GPT-2 tokenizer has no default pad_token, so set it to eos_token\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Tokenization function\n",
        "def tokenize_fn(example):\n",
        "    prompt = f\"Q: {example['instruction']}\\nA: {example['output']}\"\n",
        "    tok = tokenizer(\n",
        "        prompt,\n",
        "        truncation=True,\n",
        "        padding='max_length',\n",
        "        max_length=512,\n",
        "        return_tensors=None  # important for datasets.map compatibility\n",
        "    )\n",
        "    # For causal LM training, labels are same as input_ids\n",
        "    tok['labels'] = tok['input_ids'].copy()\n",
        "    return tok\n",
        "\n",
        "# Apply tokenization to the dataset\n",
        "tokenized = ds.map(tokenize_fn)\n",
        "\n",
        "# Print one sample from tokenized dataset\n",
        "print('Tokenized dataset sample:')\n",
        "print(tokenized[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f-hr_9tyNvXf"
      },
      "outputs": [],
      "source": [
        "def ask_question(q):\n",
        "    inputs = tokenizer(f\"Q: {q}\\nA:\", return_tensors=\"pt\").to(model.device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(**inputs, max_new_tokens=50)\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rREcqhF0N1Rw"
      },
      "outputs": [],
      "source": [
        "print(\"\\nüí¨ Chatbot BEFORE LoRA:\")\n",
        "print(ask_question(\"what is policy?\"))\n",
        "#print(ask_question(\"Does home insurance cover earthquakes?\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ej0FLNUSo-Ql"
      },
      "outputs": [],
      "source": [
        "lora_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias='none',\n",
        "    task_type='CAUSAL_LM'\n",
        ")\n",
        "model = get_peft_model(model, lora_config)\n",
        "print('LoRA layers added. Model is ready for training.')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p63zvlavpARq"
      },
      "outputs": [],
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=os.path.join(BASE_DIR, 'result'),\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=4,\n",
        "    warmup_steps=5,\n",
        "    max_steps=30,\n",
        "    learning_rate=2e-4,\n",
        "    fp16=True,\n",
        "    logging_steps=5,\n",
        "    save_strategy='no'\n",
        ")\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized,\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "# Uncomment the next line to start training in Colab (may take long)\n",
        "# trainer.train()\n",
        "print('Trainer configured. To run training, uncomment trainer.train()')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y-CKkhc6pEFf"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "# ------------ setup -----------\n",
        "\n",
        "BASE_DIR = '/content/chatbot_delivery'\n",
        "os.makedirs(os.path.join(BASE_DIR, 'result'), exist_ok=True)\n",
        "\n",
        "model_name = 'mistralai/Mistral-7B-Instruct-v0.2'\n",
        "\n",
        "# Load tokenizer with token (change token here)\n",
        "from huggingface_hub import login\n",
        "login(\"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\")  # ‡™§‡™Æ‡™æ‡™∞‡´Å‡™Ç HF ‡™ü‡´ã‡™ï‡™® ‡™Ö‡™π‡´Ä‡™Ç ‡™Æ‡´Ç‡™ï‡™µ‡´Å‡™Ç\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=True)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Load model (4-bit quantization, auto device map)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    load_in_4bit=True,\n",
        "    device_map='auto'\n",
        ")\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model.to(device)\n",
        "\n",
        "# --------- Answer generate function ------------\n",
        "\n",
        "def generate_answer(model, tokenizer, question, device=device):\n",
        "    prompt = f\"Q: {question}\\nA:\"\n",
        "    inputs = tokenizer(prompt, return_tensors='pt').to(device)\n",
        "    outputs = model.generate(**inputs, max_new_tokens=64)\n",
        "    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return answer\n",
        "\n",
        "# ------------ Manual question testing ------------\n",
        "\n",
        "# ‡™Ö‡™π‡´Ä‡™Ç manual user question ‡™≤‡™ñ‡´ã:\n",
        "user_question = input(\"Enter your question: \")\n",
        "\n",
        "answer = generate_answer(model, tokenizer, user_question, device=device)\n",
        "\n",
        "print(\"\\nUser Question:\", user_question)\n",
        "print(\"Model Answer:\", answer)\n",
        "\n",
        "# ----------- (Optional) Save example results ------------\n",
        "\n",
        "example_results = [\n",
        "  {\n",
        "    \"question\": user_question,\n",
        "    \"answer\": answer\n",
        "  }\n",
        "]\n",
        "with open(os.path.join(BASE_DIR, 'result', 'example_answer.json'), 'w') as f:\n",
        "    json.dump(example_results, f, indent=2)\n",
        "print(f\"Saved your Q&A to {os.path.join(BASE_DIR, 'result', 'example_answer.json')}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "80ZSqzJzpE5x"
      },
      "outputs": [],
      "source": [
        "user_question = input(\"Enter your question: \")\n",
        "\n",
        "answer = generate_answer(model, tokenizer, user_question, device=device)\n",
        "\n",
        "print(\"\\nUser Question:\", user_question)\n",
        "print(\"Model Answer:\", answer)\n",
        "\n",
        "# ----------- (Optional) Save example results ------------\n",
        "\n",
        "example_results = [\n",
        "  {\n",
        "    \"question\": user_question,\n",
        "    \"answer\": answer\n",
        "  }\n",
        "]"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
